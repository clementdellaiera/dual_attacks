\section{To do}

How to sample dual vectors? It seems the fashionable way consists of applying BKZ reduction to the lattice basis, and keeping the first $k$ vectors, with $k$ a suitable parameter. The sample consists then of all vectors 
$$\sum x_i b_i^{\vee}$$
with $-a \leq x_i \leq a$ (with $a$ again a suitable parameter).  

What if we randomly sample from short vectors in $\Lambda^\vee$? Of simulate a random walk 
$$\xi_{k+1} = \xi_k +\varepsilon_k$$
where $\varepsilon_k$ follows a uniform distribution on the set of first BKZ-reduced vectors of a dual basis? Maybe the test statistics is more prone to analysis via ergodic theorems? (Instead of using a central limit theorem on sample lacking independance).

Why does Pouly-Shen's paper have no restriction on the dimension of the lattice ?

Can we find a set of parameters in the contradictory regime of Ducas-Pulles with an attack that works anyway ?

The paper of Carrier et al. gives an analytical derivation of the curves of Ducas-Pulles. Are we sure these curves explained the 'in-vitro experiment' of DP without explaining anything about the attack?

\section{Results on lattices}

Concentration inequalities for lipschitz functions on discrete gaussian on lattices.

Distribution of $\lambda_1(\Lambda)$ for $\Lambda \sim \mu$, where $\mu$ is the unique inariant provavility measure on $GL(\mathbb Z ) \backslash GL(n,\mathbb R) / O(n) $.

\subsection{Independance}

For a subset $W\subset \Lambda^\vee$, let us set $X_W = (e^{2 i \pi \langle w , t \rangle})_{w\in W}$. All the previous work on the dual attacks seems to use the statistic
$$\frac{1}{|W|} \sum_{w\in W } \cos( 2\pi \langle w , t\rangle ) $$
and derive statistical tests based on either asymptotic confidence intervals relying on the Central Limit Theorem (\cite{ducas2023does}) or the Hoeffding inequality (\cite{pouly2023provable}). All these methods rely on the assumption that coordinates of $X_W$ are independent, which is not the case\footnote{More formally, they all belong to the algebra of measurable functions w.r.t. the $\sigma$-algebra generated by $t$.}: they are all function of the same variable $t$.

Yet, if one draws different samples of $X_W$, one will directly detect a different behaviour depending on the law of $t$. It might be that the coordinates are not far from being independant (if the set $W$ is chosen well). 

In order to understand the phenomenon at hand, here is a toy model : let $U_0, U_1$ be random variables following a uniform law on $(0,1)$ and a gaussian law $\mathcal N(0,s)$ respectively, and $b$ be a random Bernoulli varaible of parameter $p$. Set 
$$X_n = e^{2i\pi n U_b}\quad \forall n \in \mathbb N_{>0}. $$
Then the variables are dependent, yet uncorrelated conditionnaly to the event $\{b = 0\}$.      

For two real valued random variables $X$ and $Y$, the statistic 
$$ n^{\frac{1}{2}}\sup_{x,y}|\hat F_n^{(X,Y)}(x,y) - \hat F_n^{(X)}(x) \hat F_n^{(Y)}(y)|$$
provides a test of independance. We thus could offer (one has to do the numerical experiments) a reasonable explanation as to why the coordinates of $X_W$ seems independant by computing the $p$-value of the test (for the toy model as well).

While not independent, the coordinates of the random vector $X_W$ have a distribution depending on that of $t$. In particular, one can hope to distinguish their laws under the hypothesis where $t$ is drawn according to a LWE-distribution against the alternative where it is drawn as a uniform random variable on a fundamental domain of the lattice. We design a non-parametric test 
$$ H_0 : \nu_W = \mu \quad \text{vs} \quad H_1 : \nu_W \neq \mu $$ 
where $\nu_W $ is the distribution of $X_W$ and $\mu$ the uniform probability on the circle. 

Let $F_0$ be the cumulative distribution function of the random variable $\cos(2\pi U)$ where $U\sim \mathcal U(0,1)$, $\hat F_W$ the empirical cumulative distribution of the coordinates of $X_W$, and
$$T_W = |W|^{\frac{1}{2}}\sup |\hat F_W(x) - F_0(x)|.$$
If $|W|$ is big enough, the statistic ... no this assumes independance of the $\langle w , t \rangle $ again!

As for implementation, the $\sup$-norm is quite impractical to compute, thus one might rely on the Cramér-Von Mises statistic
$$T^2 = \int |\hat F_n(x) -F(x)|^2 dF(x),$$
Another idea would be to try and find the maximum of 
$$T_x = |\hat F_n(x) -F(x)|^2 $$
by gradient descent. As numerical experiments show, $\cos(2 \pi\langle w , t\rangle)$ have a distribution very close to $\cos(2\pi U)$ when $t$ follows a uniform law, whereas it follows a distribution concentrated around $1$ in the LWE-case. One might thus follow the following algorithm.

\begin{center}\fbox{\begin{minipage}{7cm}
Set $x = \frac{1}{2}$, $\varepsilon$ such that $T > \varepsilon$ is a rejection domain of desired risk, $N_{max}$ the limit number of loops.\\
Compute $T_x$. \\
Loop for $N_{max}$ times:\\
If $T_x > \varepsilon$, return $H_1$, break\\
Else $x \leftarrow x +\eta\nabla T_x$\\
End loop\\
Return $H_0$.
\end{minipage}}
\end{center}

As a final remark : it could be a good idea to replace the circle $\mathbb S^1 = \{z\in \mathbb C : |z|=1\}$ and the exponential, cosine and sine functions by $\mathbb R / \mathbb Z$ (identified as $(-\frac{1}{2} , \frac{1}{2})$) and the floor function, as to avoid numerical errors coming from the use of transendental functions and numbers such as $\pi$. That would mean looking at the cumulative distribution function\footnote{For a real number $x$, $\mathcal M(x)$ denotes the mantissa of $x$, i.e. the unique real number $\mathcal M(x)\in [0,1)$ such that $x-\mathcal M(x)\in \mathbb Z$.} of 
$$Y_n = \mathcal M ( n U_b +\frac{1}{2} ) -\frac{1}{2} $$
to distinguish the cases $b=0$ and $b=1$. Since when $b=1$, the $Y_n$'s accumulate at 0 while being uniformly distributed when $b=0$, one could test the uniform repartition of $Y_n$'s in  $(-\frac{1}{2} , \frac{1}{2})$ outside of a small centered set.


\subsection{Scatter points}

Principal component analysis allows to compare random data sets : with $X_W(t) = \{ \langle w , t \rangle \}_{w\in W}$, one can study and reduce the covariance matrix
$$X_W(t) X_W(t)^T =\sum_j \lambda_j u_ju_j^t.$$
If $W$ is a BKZ-reduced basis for $\Lambda^\vee$, then 
$$ \lambda_j \sim \lambda_j(\Lambda^\vee).$$

Can we prove something like $\max_j \lambda_j \leq \max_W \|w\|_2^2$?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Concentration inequalities} %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The following argument shows that the Independance Heuristic of \cite{} is not needed in the study of the total score. The derivation of the bounds for the score is also flawed by the use of the Central Limit theorem, an asymptotic result. An exact control needs an extra computation for the distance between the distribution of the score and the limit normal distribution. This was already pointed out in \cite{}, in which the use of Hoeffding inequality provided exact (and more precise) confidence intervals. We improve on this idea of using exact deviation inequalities by using concentration results for Lipschitz functions, which does not assume independance of the comonent of the score.     

Recall the toy model $X_n = e^{2i\pi nU}$ with $U\sim \mathcal U(0,1)$. If $f : \mathbb R \rightarrow \mathbb C$ is any $1$-periodic function such that $\int_0^1 |f|^2$ is finite, then $f = \sum c_n e_n $ in $L^2$. The random variable $\sum c_n e_n(U)$ converges in $L^2$, and can take any value of $f$. This suggests that a universal bound for arbitrary sums of uncorrelated variables is unlikely. More precisely, if $f(x) = \ sg(x)\sum_k k \chi_{x < k^{-1} } $ when $x\in (-\frac{1}{2},\frac{1}{2})$, and $1$ periodic, then $\mathbb E[f(U)] = 0$ and 
$$\sup_f \frac{1}{N^2}\log \mathbb P(|f(U)| > N) = +\infty .$$ 
One can still control deviations of sums on non-independant variables using the following theorem (a proof can be found in \cite{vershynin2018high}, see theorem 5.1.4 page 99). We denote the vector space $\mathbb R^n$ with euclidean norm $\|\cdot\|_2$ by $\ell^2_n$.

\begin{theorem}
Let $X$ be a random variable distributed either uniformly on the sphere of $\ell^2_n$ of radius $\sqrt n$, or as a centered gaussian vector $\mathcal N_{\ell^2_n}(0, Id_n)$, or as a uniform variable on $(0,1)^n$. Then 
$$ \mathbb  P\left(|f(X) -\mathbb E[f(X)]| \geq t\right) \leq 2\exp{\left(-\frac{ct^2}{\|f\|^2_{Lip}} \right)}$$
\end{theorem}

Let $W\subset \Lambda^\vee$ be a finite subset, and define 
$$f_W(t) = \frac{1}{|W|}\sum_{w\in W} \cos(2\pi \langle w , t \rangle) \quad \forall t \in \ell^2_n.$$
Let us denote by $L$ the positive number $2\pi\max_{w\in W} \|w\|_2$. Then
\[\begin{split}
|f_W(x) -f_W(y)| & \leq \frac{1}{|W|}\sum_{w\in W} |\cos(2\pi \langle w , x \rangle ) -\cos(2\pi \langle w , y \rangle) | \\
		 & \leq \frac{2\pi}{|W|}\sum_{w\in W} \|w\|_2 \|x-y\|_2\\
		 & \leq L\|x-y\|_2\\ 
\end{split}\]
i.e. $f_W$ is a $L$-Lipschitz function.  

If $X$ follows a LWE-distribution, then $\mathcal E[f_W(X)] \sim 1$ (to prove) and the inequality above says that $f_W(X)$ is close to one with high probability.

If $X$ follows a uniform distribution on the fundamental domain, prove that $\mathbb E[f_W(X)]= 0$ and that $f_W(X)$ lies outside of a neighborhood of $0$ with high probability.

\subsection{Bounds for discrete gaussian distributions}

In order to be able to apply the exact bounds for a LWE distinguisher, we prove concentration inequalities valid for discrete gaussians. Recall our notation for the gaussian function 
$$\rho_s(v)=\exp(-\pi \frac{\pi}{s^2}\|v\|^2 )\quad \forall v\in \mathbb R^n.$$
The continuous gaussian measure\footnote{We denoted the Lebsgue measure of dimension $d$ by $m_d$.} is 
$$\gamma_s(A) =  \int_A s^{-n}\rho_s(x)dm_d(x) ,$$
whereas the discrete gaussian on $\Lambda$ is the probability measure 
$$D_{s,\Lambda} = \rho(\Lambda )^{-1} \sum_{v\in \Lambda} \rho_s(v)\delta_v. $$

\begin{proposition}
Let $\xi \sim D_{s,\Lambda}$ and $r > \lambda_d^{(\Lambda)}$, then 
$$\mathbb P(\|\xi \|> r )\leq 2^de^{-\frac{\pi}{s^2} C_\Lambda \tilde r^2}$$
where $C_\Lambda$ is a constant that only depends on the lattice and $\tilde r$ is the closest integer to $\frac{r}{\lambda_d^{(\Lambda)}}$.
\end{proposition} 

\begin{proof}
Let $\{v_k\}$ be a reduced basis for $\Lambda$, i.e. $\|v_k\| = \lambda_k^{(\Lambda)}$. For $a\in \mathbb N^d$, let $\Lambda_a$ denote the sublattice of $\Lambda$ generated by $\{a_kv_k\}$  and let us set 
$$\mathcal V_a = \{t\in E : \|t\|\leq \|t-v\| \forall v\in \Lambda_a\}, $$
a Voronoi cell for $\Lambda_a$. We will need to use, for a vector $t\in E$, the half-space 
$$H_t = \{y \in E : \langle y-t , t\rangle \leq 0  \}$$ 
defined by vectors positively oriented with respect to the median hyperplane between $t$ and the origin. If $\Lambda_{t} =\Lambda\cap H_t$, one has the disjoint union
$$\Lambda = \Lambda_t \coprod \Lambda_{-t} \coprod \Lambda_t^{(0)}.$$
If $u\in \Lambda$, one has
\[\begin{split}
\rho_s(\Lambda_u) & = e^{-\frac{\pi}{s^2} \|u\|^2 }\sum_{v\in \Lambda , \langle v-u ,u\rangle >0 } e^{-\frac{\pi}{s^2} \|v - u \|^2}e^{-\frac{\pi}{s^2} \langle v-u,u\rangle} \\
		& \leq  e^{-\frac{\pi}{s^2} \|u\|^2 } \rho_s(\Lambda) \\ 
\end{split}.\]
This ensures that $\rho_s(\Lambda \backslash \Lambda_u^{(0)}) \leq 2e^{-\frac{\pi}{s^2} \|u\|^2} \rho_s(\Lambda) $. By induction, we get that
$$\rho_s(\Lambda \cap \mathcal V_a^c) \leq 2^d  e^{-\frac{\pi}{s^2} \sum_{k} \|a_ku_k\|^2}\rho_s(\Lambda) ,$$
i.e.
$$\mathbb P(\xi \in \mathcal V_a^c) \leq  2^d e^{-\frac{\pi}{s^2} \sum a_k^2\lambda_k^2} .$$
If $r\geq \lambda_d^{(\Lambda)}$, let $a$ be maximal such that $\mathcal V_a\subset B(0,r)$, thus
$$\mathbb P(\| \xi \| \geq r) \leq  2^d\prod_{k} 2e^{-\frac{\pi}{s^2} a_k^2\lambda_k^2} .$$
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Best score and p-value} %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For $c >0 $, let us denote 
$$\mathcal R_{t,c} = \{f_W(t) > c\}$$
a rejection region defining a statistical test. Choosing $t$ that maximizes $f_W(t)$ is equivalent to selecting the statistical test with minimal p-value. Indeed, let $t_0 = \text{argmax }f_W(t)$ and $\alpha^*_t$ be the p-value of the test associated with $\mathcal R_{t,c}$, and $c^* = f_W(t_0)$. Then 
$$\alpha^*_{t_0} \leq  P(\mathcal R_{t_0,c^*} ) \leq \alpha^*_t \quad \forall t\neq t_0.$$


